import bs4, os, sys, requests
from urllib.parse import urljoin

domain = sys.argv[1]


with open(f'recon/{domain}/crawler_output', 'w') as file:
    pass # as the write option has already been defined


session = requests.Session()

def request(url):
    try:
        html = requests.get(url, allow_redirects=False, timeout=2, cookies={})
        return html.content
    except Exception as e:
        print(e)
        return ''


content_list = []


def crawl(url):
    try:
        html = request (url)
        soup = bs4.BeautifulSoup(html, 'html.parser')
        for a in soup.find_all('a', href=True):
            link = urljoin(url, a['href'])

            if '#' in link:
                link = link.split('#')[0]

            if link not in content_list and domain in link and "dvwa" in link:
                content_list.append(link)
                print("[+] Found the url: {}".format(link))

                with open('recon/{domain}/crawler_output', 'a') as file:
                    file.write(link + '\n')

                crawl(link)
    except KeyboardInterrupt:
        exit(0)


with open('recon/{domain}/subdomains', 'r') as file:
    subdomains = file.read().splitlines()
    for subdomain in subdomains:
        url = f"https://{subdomain}"
        crawl(url)